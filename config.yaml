## Debug
debug: false
## Prints URL in terminal
display_matched_url: false

## Name of the project
project: "Crawler Project"
# Type of project (crawl / batch)
project_type: "crawl"
# Directory to store downloaded content
directory: "Project"
# Number of workers
web_count: 100

### Crawler settings
# First URL to crawl
root_url: "http://example.com/"
# Regex to validate if link has to be crawled
link_validator: "^http://example.com/.*"
# Regex to preprocess the link before queuing for example (#.*) will select all characters after # in the url
link_sanitizer: "(#.*)"
# Replacement for selected string from sanitizer
link_sanitizer_replacement: ""

### Batch processing settings
# Batch URL example: https://www.example.com/[$01-$999]/[$1-$9]/sjkn
# [$1-$999] will loop from 1 - 999
# [$01-$999] will loop from 01 - 999
# [$001-$099] will loop from 001 - 099
# It will be ignored if project_type is not batch
batch_url: ""

### Parser settings
# Check for the content in the page only if this css selector is present, keep it blank to ignore this
page_validator: ""
# CSS Selector of the content to be parsed
content_selector: ".storyBody"

# This will determine if text will be fetched from selected tags or an attribute of tag will be selected (text/attr/html)
content_holder: "attr"
# Optional if content_holder is set to text otherwise it will be ignored
content_tag_attr: "src"

# To use paid proxy API, URL will be appended to it before fetching. Leave it blank otherwise
# proxy_api: "http://api.scraperapi.com?api_key=YOURKEYHERE&url="
proxy_api: ""
